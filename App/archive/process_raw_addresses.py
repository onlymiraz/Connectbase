from datetime import date

import pandas as pd
import pandas_usaddress as padd


def clean_addresses(df_l, df_r):
    """
     Cleans addresses for maximum accuracy in matching between two tables.
     Steps:
        * Removes .0 from zip codes
        * Removes NA words like "missing" (must be exact, not partial)
        * Removes tab or 1+ empty spaces when used as filler
        * Replaces NaN with '' for string concatenation
        * All lower case
        * Concatenated into single field
        * Drops nulls
        * Drop empties

    Args:
        df_l (pd.DataFrame): The first table to be address matched.
        df_r (pd.DataFrame): The second table to be address matched.

    Returns:
        pd.DataFrame: Same df_l with cleaned addresses in the full_address column.
        pd.DataFrame: Same df_r with cleaned addresses in the full_address column.
    """
    # remove .0 from zip codes (python think it's a float)
    df_l['ZIP'] = df_l['ZIP'].astype(str).str.replace('.0', '')
    df_r['ZIP'] = df_r['ZIP'].astype(str).str.replace('.0', '')

    # both tables both have same 4 address column names
    # will need to change if future tables differ
    # also applies to above (ex: zip_code database field)
    lst_address_cols = ['ADDRESS', 'CITY', 'STATE', 'ZIP']
    lst_na_words = ['undefined', 'unknown', 'blank', 'missing', 'pending',
                    'none', 'null', 'void', 'unavailable', 'unspecified',
                    'undetermined', 'tbd', 'na', 'n/a', 'nan', '<na>', '	',
                    '.', ',', '-', '_', '?', 'n/r', 'n/e']

    # replace missing and manual bad data entry with empty space
    for address_col in lst_address_cols:
        # for every address column:

        # replace NaN with nothing
        df_l[address_col].fillna('', inplace=True)
        df_r[address_col].fillna('', inplace=True)

        # for every NA substitute word:
        # replace it with nothing
        for NA_word in lst_na_words:
            df_l.loc[df_l[address_col].str.lower() == NA_word, address_col] = ''
            df_r.loc[df_r[address_col].str.lower() == NA_word, address_col] = ''

    # concat fields into one, make lowercase, and strip empty cells of whitespace(s)
    df_l['full_address'] = (
            df_l['ADDRESS'].astype(str).str.lower().str.strip() + ' '
            + df_l['CITY'].astype(str).str.lower().str.strip() + ' '
            + df_l['STATE'].astype(str).str.lower().str.strip() + ' '
            + df_l['ZIP'].astype(str).str.lower().str.strip())

    df_r['full_address'] = (
            df_r['ADDRESS'].astype(str).str.lower().str.strip() + ' '
            + df_r['CITY'].astype(str).str.lower().str.strip() + ' '
            + df_r['STATE'].astype(str).str.lower().str.strip() + ' '
            + df_r['ZIP'].astype(str).str.lower().str.strip())

    # for safety, drop nulls to prevent bad matching between tables
    # all nulls will cartesian join each other and explode the results
    df_l = df_l.dropna(subset=['full_address'])
    df_r = df_r.dropna(subset=['full_address'])

    # also drop empty values
    df_l = df_l.loc[df_l['full_address'] != '', :]
    df_r = df_r.loc[df_r['full_address'] != '', :]

    # keep track of index for future matching
    df_l['index'] = df_l.reset_index().index
    df_r['index'] = df_r.reset_index().index

    return df_l, df_r


def parse_addresses(df_l, df_r):
    """
     1. Takes full_address columns and tags as individual address fields.
     2. Joins in various ways to carefully maximize findings.

    Args:
        df_l (pd.DataFrame): The first table to be address matched.
        df_r (pd.DataFrame): The second table to be address matched.

    Returns:
        pd.DataFrame: A dataframe with matching addresses.
    """
    # 1. TAGGING
    # this is the hard computation of going through and standardizing
    # new df is old one + new address columns
    df_l_usaddr = padd.tag(df_l,
                           ['full_address'],
                           granularity='high',
                           standardize=True)

    df_r_usaddr = padd.tag(df_r,
                           ['full_address'],
                           granularity='high',
                           standardize=True)

    # list of new columns generated by standardizing the addresses
    lst_padd_fields = ['AddressNumber', 'BuildingName', 'OccupancyType',
                       'OccupancyIdentifier', 'PlaceName', 'Recipient',
                       'StateName', 'StreetName', 'StreetNamePreDirectional',
                       'StreetNamePreModifier', 'StreetNamePreType',
                       'StreetNamePostDirectional', 'StreetNamePostModifier',
                       'StreetNamePostType', 'SubaddressIdentifier',
                       'SubaddressType', 'USPSBoxID', 'USPSBoxType', 'ZipCode']

    # drop rows where tag failed for all columns
    df_l_usaddr.dropna(how='all', subset=lst_padd_fields, inplace=True)
    df_r_usaddr.dropna(how='all', subset=lst_padd_fields, inplace=True)

    # # also if all new tags are blank values drop rows
    # def check_match(row):
    #     return all(row[col] == '' for col in lst_padd_fields)
    #
    # # apply the function to each row and drop those that are all blank
    # df_l_usaddr = df_l_usaddr[df_l_usaddr.apply(check_match, axis=1)]
    # df_r_usaddr = df_r_usaddr[df_r_usaddr.apply(check_match, axis=1)]

    # strangely, new cols have inconsistent dtypes; must be fixed
    for i in lst_padd_fields:
        df_l_usaddr[i] = df_l_usaddr[i].astype(str)
        df_r_usaddr[i] = df_r_usaddr[i].astype(str)

    abbrev_state = {'alabama': 'al', 'alaska': 'ak', 'arizona': 'az', 'arkansas': 'ar', 'california': 'ca',
                    'colorado': 'co', 'connecticut': 'ct', 'delaware': 'de', 'florida': 'fl',
                    'georgia': 'ga', 'hawaii': 'hi', 'idaho': 'id', 'illinois': 'il', 'indiana': 'in',
                    'iowa': 'ia', 'kansas': 'ks', 'kentucky': 'ky', 'louisiana': 'la', 'maine': 'me',
                    'maryland': 'md', 'massachusetts': 'ma', 'michigan': 'mi', 'minnesota': 'mn',
                    'mississippi': 'ms', 'missouri': 'mo', 'montana': 'mt', 'nebraska': 'ne', 'nevada': 'nv',
                    'new hampshire': 'nh', 'new jersey': 'nj', 'new mexico': 'nm', 'new york': 'ny',
                    'north carolina': 'nc', 'north dakota': 'nd', 'ohio': 'oh', 'oklahoma': 'ok', 'oregon': 'or',
                    'pennsylvania': 'pa', 'rhode island': 'ri', 'south carolina': 'sc', 'south dakota': 'sd',
                    'tennessee': 'tn', 'texas': 'tx', 'utah': 'ut', 'vermont': 'vt', 'virginia': 'va',
                    'washington': 'wa', 'west virginia': 'wv', 'wisconsin': 'wi', 'wyoming': 'wy'}

    def convert_state(state):
        if len(state) == 2:
            # if already a 2 letter abbreviation, return it
            return state
        elif state in abbrev_state:
            # if full state name, return the abbreviation
            return abbrev_state[state]
        else:
            # if no state found, return an empty string
            return ''

    df_l_usaddr['StateName'] = df_l_usaddr['StateName'].apply(convert_state)
    df_r_usaddr['StateName'] = df_r_usaddr['StateName'].apply(convert_state)

    # 2. MATCHING
    # match on various criteria: from 2 tables to 1
    # WARNING - results will vary! columns selected for matching on is critical

    df_l_usaddr = df_l_usaddr.add_suffix('_l')
    df_r_usaddr = df_r_usaddr.add_suffix('_r')

    # complete and exact matches on all new address fields (prone to false negatives)
    df_usaddr_match_compl = pd.merge(left=df_l_usaddr,
                                     right=df_r_usaddr,
                                     how='inner',
                                     left_on=[col + '_l' for col in lst_padd_fields],
                                     right_on=[col + '_r' for col in lst_padd_fields]
                                     )

    # basic, minimalist matching (prone to false positives)
    lst_basic = ['AddressNumber', 'StreetName', 'PlaceName', 'StateName', 'ZipCode']
    df_usaddr_match_basic = pd.merge(left=df_l_usaddr,
                                     right=df_r_usaddr,
                                     how='inner',
                                     left_on=[col + '_l' for col in lst_basic],
                                     right_on=[col + '_r' for col in lst_basic]
                                     )

    # like basic but include predirection
    lst_prdir = ['AddressNumber', 'StreetName',
                 'StreetNamePreDirectional', 'PlaceName',
                 'StateName', 'ZipCode']
    df_usaddr_match_prdir = pd.merge(left=df_l_usaddr,
                                     right=df_r_usaddr,
                                     how='inner',
                                     left_on=[col + '_l' for col in lst_prdir],
                                     right_on=[col + '_r' for col in lst_prdir]
                                     )

    # match when missing zip but do have city, state
    lst_nozip = ['AddressNumber', 'StreetName', 'StreetNamePreDirectional',
                 'PlaceName', 'StateName']
    df_usaddr_match_nozip = pd.merge(left=df_l_usaddr,
                                     right=df_r_usaddr,
                                     how='inner',
                                     left_on=[col + '_l' for col in lst_nozip],
                                     right_on=[col + '_r' for col in lst_nozip]
                                     )

    # match when missing city, state but do have zip
    lst_onzip = ['AddressNumber', 'StreetName', 'StreetNamePreDirectional', 'ZipCode']
    df_usaddr_match_onzip = pd.merge(left=df_l_usaddr,
                                     right=df_r_usaddr,
                                     how='inner',
                                     left_on=[col + '_l' for col in lst_onzip],
                                     right_on=[col + '_r' for col in lst_onzip]
                                     )

    # combine results
    df_usaddr_match = pd.concat(objs=[
        # df_usaddr_match_basic, 
        df_usaddr_match_prdir,
        df_usaddr_match_compl,
        df_usaddr_match_onzip,
        df_usaddr_match_nozip,
    ], axis=0)

    # drop duplicate match results
    df_usaddr_match.drop_duplicates(subset=['index_l', 'index_r'], inplace=True)

    # ensure NULL
    df_usaddr_match.fillna('NULL', inplace=True)
    df_usaddr_match.replace(r'^nan$', 'NULL', inplace=True, regex=True)

    # add new field for update date
    df_usaddr_match['MODEL_RUN'] = date.today().strftime('%Y-%m-%d')

    return df_usaddr_match

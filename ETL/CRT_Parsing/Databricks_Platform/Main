##### RUN EACH CELL IN ORDER #####

# CELL 1 #
pip install xlrd dbutils


# CELL 2 #
%restart_python


# CELL 3 #
from datetime import datetime
from io import BytesIO
import json
import os
import zipfile
import numpy as np
import pandas as pd
from pyspark.sql.functions import pandas_udf
from pyspark.sql.types import StringType
import xlrd

### SET OUTPUT PATHS ###
run_date = datetime.today().strftime("%Y-%m-%d")
output_dir_1 = f"/Volumes/wholesale/bronze/crt_output/{str(run_date)}/1_original_file/"
output_dir_2e = f"/Volumes/wholesale/bronze/crt_output/{str(run_date)}/2_raw_extract/equipment_expenses/"
output_dir_2n = f"/Volumes/wholesale/bronze/crt_output/{str(run_date)}/2_raw_extract/new_osp_build/"
output_dir_3e = f"/Volumes/wholesale/bronze/crt_output/{str(run_date)}/3_processed/equipment_expenses/"
output_dir_3n = f"/Volumes/wholesale/bronze/crt_output/{str(run_date)}/3_processed/new_osp_build/"

# make dirs if not exist
for dr in [output_dir_1,
           output_dir_2e, output_dir_2n,
           output_dir_3e, output_dir_3n]:
    try:
        # dbutils.fs.ls(dr)
        dbutils.fs.rm(dr, True)
    except Exception:
        pass
    dbutils.fs.mkdirs(dr)


### UDF TO APPLY FUNCTION TO WORKER NODES ###
@pandas_udf(StringType())
def process_and_write_excel(binary_series: pd.Series) -> pd.Series:
    def process_single_binary(file_content):
        # default is 8 builds per sheet
        lst_builds = []
        max_builds = 8
        for i in range(1, max_builds + 1):
            lst_builds.append(f'OSP Build Inquiry {i}')

        # extracted values will be stored in this dictionary
        dict_json = {}

        # read binary data and unzip
        try:
            with (zipfile.ZipFile(BytesIO(file_content), "r") as zip_ref):
                try:
                    for file_info in zip_ref.infolist():
                        if file_info is None:
                            continue
                        if file_info.filename.startswith('CRT') and file_info.filename.endswith('.xls'):
                            with zip_ref.open(file_info, 'r') as file:
                                ### SAVE EXCEL FILE AS IS ###
                                with open(os.path.join(output_dir_1, file_info.filename), 'wb') as output_file:
                                    output_file.write(file.read())

                            print("Completed write to original")
                            print("Started processing sheets")
                            with zip_ref.open(file_info, 'r') as file:

                                ### READ .XLS FILE USING XLRD LIBRARY ###
                                wb = xlrd.open_workbook(file_contents=file.read())

                                for sheet_name in wb.sheet_names():
                                    ### EQUIPMENT EXPENSES ###
                                    if sheet_name == "Equipment Expenses":
                                        print(f"Found sheet: {sheet_name}")

                                        # extract data from sheet
                                        sheet = wb.sheet_by_name(sheet_name)
                                        data = []
                                        for row in range(sheet.nrows):
                                            data.append([sheet.cell(row, col).value for col in range(sheet.ncols)])

                                        # convert data to pandas DataFrame
                                        df_equip = pd.DataFrame(data)

                                        # save raw df to csv
                                        file_path = os.path.join(output_dir_2e, file_info.filename)
                                        df_equip.to_csv(file_path, index=False)

                                        # filter equipment table
                                        try:
                                            df_equip = df_equip.iloc[4:35, 2:]

                                            new_header = df_equip.iloc[0]  # store first row for header
                                            df_equip = df_equip[1:]  # drop first row
                                            df_equip.columns = new_header  # set header to stored values
                                            df_equip = df_equip.drop(df_equip.columns[-2],
                                                                     axis=1)  # drop empty hidden column

                                            # save extracted df to csv if anything there
                                            file_path = os.path.join(output_dir_3e, f'{file_info.filename}.csv')
                                            if df_equip['Loaded Total'].sum() > 0:
                                                df_equip.to_csv(file_path, index=False)

                                        except Exception as e:
                                            print(f'Failed: {file_info.filename} - {sheet_name}: {e}')
                                            continue

                                    ### NEW OSP BUILD ###
                                    if sheet_name == "New OSP Build":
                                        print(f"Found sheet: {sheet_name}")

                                        # extract data from sheet
                                        sheet = wb.sheet_by_name(sheet_name)
                                        data = []
                                        for row in range(sheet.nrows):
                                            data.append([sheet.cell(row, col).value for col in range(sheet.ncols)])

                                        # convert data to pandas DataFrame
                                        df_osp = pd.DataFrame(data)

                                        # save raw df to csv
                                        file_path = os.path.join(output_dir_2n, file_info.filename)
                                        df_osp.to_csv(file_path, index=False)

                                        # filter OSP build inquiries
                                        try:
                                            build_id = file_info.filename.split('.csv')[0]
                                            dict_json[build_id] = {
                                                "Build_ID": build_id,
                                                "Special_Material_Loading_Pct": df_osp.iloc[3, 9]
                                            }

                                            builds = []

                                            # build_id = file_info.filename.split('.csv')[0]
                                            # if build_id not in dict_json:
                                            #   dict_json[build_id] = {
                                            #     "Build_ID": build_id,
                                            #     "Special_Material_Loading_Pct": df_osp.iloc[3, 9]
                                            #     }

                                            data_dict = {}

                                            for index, build_number in enumerate(lst_builds):
                                                mask = df_osp.eq(build_number)
                                                iloc = np.where(mask)[0][0] + 1, np.where(mask)[1][0] + 1
                                                engineer_name = df_osp.iloc[iloc[0], iloc[1]]
                                                if engineer_name == "":
                                                    continue

                                                # Get the rows below the "OSP Build Inquiry X" cell
                                                rows_below = df_osp.iloc[iloc[0] + 1:, iloc[1] - 1]

                                                # Find the first match of "Material:"
                                                mask_material = rows_below.eq('Material:')
                                                iloc_material = np.where(mask_material)[0][0] + iloc[0] + 1

                                                # Find the first match of "Placing and Engineering:"
                                                mask_placing = rows_below.eq('Placing and Engineering:')
                                                iloc_placing = np.where(mask_placing)[0][0] + iloc[0] + 1

                                                # Find the first match of "Splicing:"
                                                mask_splicing = rows_below.eq('Splicing:')
                                                iloc_splicing = np.where(mask_splicing)[0][0] + iloc[0] + 1

                                                # Find the first match of "Removal:"
                                                mask_removal = rows_below.eq('Removal:')
                                                iloc_removal = np.where(mask_removal)[0][0] + iloc[0] + 1

                                                material_data = {}
                                                placing_data = {}
                                                splicing_data = {}
                                                removal_data = {}

                                                # Loop through the rows below the "Material:" cell
                                                material_subtotal = 0
                                                material_loadings = None
                                                for i in range(1, 100):  # arbitrary large number
                                                    if (df_osp.iloc[iloc_material + i, iloc[1]] == "" and
                                                            df_osp.iloc[iloc_material + i + 1, iloc[1]] == ""):
                                                        break
                                                    elif df_osp.iloc[iloc_material + i, iloc[1]] == "":
                                                        if material_loadings is None:
                                                            material_loadings = df_osp.iloc[
                                                                iloc_material + i, iloc[1] + 4]
                                                        continue
                                                    material_data[f"Material_Description_{i}"] = df_osp.iloc[
                                                        iloc_material + i, iloc[1]]
                                                    material_data[f"Material_Unit_{i}"] = df_osp.iloc[
                                                        iloc_material + i, iloc[1] + 1]
                                                    material_data[f"Material_Quantity_{i}"] = df_osp.iloc[
                                                        iloc_material + i, iloc[1] + 2]
                                                    material_data[f"Material_Cost_{i}"] = df_osp.iloc[
                                                        iloc_material + i, iloc[1] + 3]
                                                    material_data[f"Material_Total_{i}"] = round(
                                                        df_osp.iloc[iloc_material + i, iloc[1] + 4],
                                                        2)
                                                    material_subtotal += round(
                                                        pd.to_numeric(df_osp.iloc[iloc_material + i, iloc[1] + 4],
                                                                      errors='coerce'),
                                                        2)

                                                    # After the last material description is confirmed
                                                    last_material_row = iloc_material + i - 1

                                                material_total = material_subtotal
                                                if material_loadings is not None:
                                                    material_total += material_loadings

                                                # Search down that column for the first instance of the word "Loadings %"
                                                if 'material_subtotal' in locals():
                                                    loadings_col = iloc[1]
                                                    for j in range(last_material_row + 1, len(df_osp)):
                                                        if df_osp.iloc[j, loadings_col] == "Loadings %":
                                                            # Retrieve the value 1 right of that
                                                            material_loadings = df_osp.iloc[j, loadings_col + 1]
                                                            break
                                                else:
                                                    material_loadings = 0

                                                # Recalculate material_total
                                                material_total = material_subtotal + (
                                                        material_loadings * material_subtotal)

                                                # Loop through the rows below the "Placing and Engineering:" cell
                                                placing_total = 0
                                                for i in range(1, 100):  # arbitrary large number
                                                    if (df_osp.iloc[iloc_placing + i, iloc[1]] == "" and
                                                            df_osp.iloc[iloc_placing + i + 1, iloc[1]] == ""):
                                                        break
                                                    elif df_osp.iloc[iloc_placing + i, iloc[1]] == "":
                                                        continue
                                                    placing_data[f"Placing_Description_{i}"] = df_osp.iloc[
                                                        iloc_placing + i, iloc[1]]
                                                    placing_data[f"Placing_Unit_{i}"] = df_osp.iloc[
                                                        iloc_placing + i, iloc[1] + 1]
                                                    placing_data[f"Placing_Quantity_{i}"] = df_osp.iloc[
                                                        iloc_placing + i, iloc[1] + 2]
                                                    placing_data[f"Placing_Cost_{i}"] = df_osp.iloc[
                                                        iloc_placing + i, iloc[1] + 3]
                                                    placing_data[f"Placing_Total_{i}"] = round(
                                                        df_osp.iloc[iloc_placing + i, iloc[1] + 4],
                                                        2)
                                                    placing_total += round(
                                                        pd.to_numeric(df_osp.iloc[iloc_placing + i, iloc[1] + 4],
                                                                      errors='coerce'),
                                                        2)

                                                # Loop through the rows below the "Splicing:" cell
                                                splicing_total = 0
                                                for i in range(1, 100):  # arbitrary large number
                                                    if (df_osp.iloc[iloc_splicing + i, iloc[1]] == "" and
                                                            df_osp.iloc[iloc_splicing + i + 1, iloc[1]] == ""):
                                                        break
                                                    elif df_osp.iloc[iloc_splicing + i, iloc[1]] == "":
                                                        continue
                                                    splicing_data[f"Splicing_Description_{i}"] = df_osp.iloc[
                                                        iloc_splicing + i, iloc[1]]
                                                    splicing_data[f"Splicing_Unit_{i}"] = df_osp.iloc[
                                                        iloc_splicing + i, iloc[1] + 1]
                                                    splicing_data[f"Splicing_Quantity_{i}"] = df_osp.iloc[
                                                        iloc_splicing + i, iloc[1] + 2]
                                                    splicing_data[f"Splicing_Cost_{i}"] = df_osp.iloc[
                                                        iloc_splicing + i, iloc[1] + 3]
                                                    splicing_data[f"Splicing_Total_{i}"] = round(
                                                        df_osp.iloc[iloc_splicing + i, iloc[1] + 4],
                                                        2)
                                                    splicing_total += round(
                                                        pd.to_numeric(df_osp.iloc[iloc_splicing + i, iloc[1] + 4],
                                                                      errors='coerce'),
                                                        2)

                                                # Loop through the rows below the "Removal:" cell
                                                removal_total = 0
                                                for i in range(1, 100):  # arbitrary large number
                                                    if (df_osp.iloc[iloc_removal + i, iloc[1]] == "" and
                                                            df_osp.iloc[iloc_removal + i + 1, iloc[1]] == ""):
                                                        break
                                                    elif df_osp.iloc[iloc_removal + i, iloc[1]] == "":
                                                        continue
                                                    removal_data[f"Removal_Description_{i}"] = df_osp.iloc[
                                                        iloc_removal + i, iloc[1]]
                                                    removal_data[f"Removal_Unit_{i}"] = df_osp.iloc[
                                                        iloc_removal + i, iloc[1] + 1]
                                                    removal_data[f"Removal_Quantity_{i}"] = df_osp.iloc[
                                                        iloc_removal + i, iloc[1] + 2]
                                                    removal_data[f"Removal_Cost_{i}"] = df_osp.iloc[
                                                        iloc_removal + i, iloc[1] + 3]
                                                    removal_data[f"Removal_Total_{i}"] = round(
                                                        df_osp.iloc[iloc_removal + i, iloc[1] + 4],
                                                        2)
                                                    removal_total += round(
                                                        pd.to_numeric(df_osp.iloc[iloc_removal + i, iloc[1] + 4],
                                                                      errors='coerce'),
                                                        2)

                                                # data_dict[f"Build_{index + 1}"] = {
                                                #     "Engineer": engineer_name,
                                                #     "Contact": df_osp.iloc[iloc[0], iloc[1] + 2],
                                                #     "Description": df_osp.iloc[iloc[0] + 2, iloc[1]],
                                                #     **material_data,
                                                #     "Material_Subtotal": material_subtotal,
                                                #     "Material_Loadings": material_loadings,
                                                #     "Material_Total": material_total,
                                                #     **placing_data,
                                                #     "Placing_Total": placing_total,
                                                #     **splicing_data,
                                                #     "Splicing_Total": splicing_total,
                                                #     **removal_data,
                                                #     "Removal_Total": removal_total,
                                                # }
                                                data_dict = {
                                                    "Engineer": engineer_name,
                                                    "Contact": df_osp.iloc[iloc[0], iloc[1] + 2],
                                                    "Description": df_osp.iloc[iloc[0] + 2, iloc[1]],
                                                    **material_data,
                                                    "Material_Subtotal": material_subtotal,
                                                    "Material_Loadings": material_loadings,
                                                    "Material_Total": material_total,
                                                    **placing_data,
                                                    "Placing_Total": placing_total,
                                                    **splicing_data,
                                                    "Splicing_Total": splicing_total,
                                                    **removal_data,
                                                    "Removal_Total": removal_total,
                                                }

                                                builds.append(data_dict)
                                                dict_json[build_id]["Builds"] = builds
                                                dict_json[build_id]["Build_Count"] = len(builds)

                                                # Update dict_json with data_dict
                                                dict_json[build_id].update(data_dict)

                                                # Update build count
                                                build_count = len(
                                                    [key for key in dict_json[build_id] if
                                                     key.startswith("Build_")]) - 1
                                                dict_json[build_id].update({"Build_Count": build_count})

                                                # Write dict_json to JSON file
                                                file_path = os.path.join(output_dir_3n, f'{file_info.filename}.json')
                                                with open(file_path, 'w') as f:
                                                    json.dump(dict_json, f)

                                        except Exception as e:
                                            print(f'Failed to update build count: {e}')
                                            print(f'Failed: {file_info.filename} - {sheet_name}: {e}')
                                            continue

                    return f"SUCCESS: Processed"

                except Exception as e:
                    return f"ERROR: {e}"

        except Exception as e:
            return f"ERROR: {e}"

    return binary_series.apply(lambda x: process_single_binary(x))


# CELL 4 #
table_name = "wholesale.bronze.crt_document"

# read catalog into a table
df = spark.sql("select * from wholesale.bronze.crt_document")

#spark.table(table_name)#.col('DOCUMENT_HOLDER')

result_df = df.withColumn("processing_status", process_and_write_excel(df["DOCUMENT_HOLDER"]))
#result_df.count()
result_df.drop("DOCUMENT_HOLDER").write.mode('overwrite').saveAsTable("wholesale.bronze.crt_document_process_status")
#display(result_df)


# CELL 5 #
# Now that json files are created, combine them to single csv

import pandas as pd
from pyspark.sql import SparkSession

# Create a SparkSession
spark = SparkSession.builder.appName("json combiner").config("spark.driver.memory", "128g").getOrCreate()
# spark.conf.set("spark.driver.memory", "64g")

# Define the directory containing the JSON files
json_dir = "dbfs:/Volumes/wholesale/bronze/crt_output/{str(run_date)}/3_processed/new_osp_build/"
# Define the output directory for the CSV files
csv_dir = "dbfs:/Volumes/wholesale/bronze/crt_output/{str(run_date)}/3_processed/combined_osp_csvs/"

# Define the batch size
batch_size = 1000

# Get a list of all JSON files in the directory
json_files = dbutils.fs.ls(json_dir)

# Process the JSON files in batches
for i in range(0, len(json_files), batch_size):
    batch_files = [file.path for file in json_files[i:i+batch_size]]
    batch_df = spark.read.json(batch_files)

    if "xls" in batch_df.columns:
        batch_df = batch_df.select("xls.*")
        batch_df = batch_df.selectExpr("explode(Builds) as Builds")
        batch_df = batch_df.select("Builds.*")
    else:
        # Handle the case where the "xls" column does not exist
        # For example, you can select all columns or a specific set of columns
        batch_df = batch_df.select("*")
        batch_df = batch_df.selectExpr("to_json(struct(*)) as json_data")

    batch_df.write.csv(f"{csv_dir}/batch_{i}.csv", header=True)

# Read all CSV files into a single DataFrame
csv_files = dbutils.fs.ls(csv_dir)
csv_dfs = [spark.read.csv(file.path, header=True) for file in csv_files]
single_df = csv_dfs[0]
for df in csv_dfs[1:]:
    single_df = single_df.union(df)

# Save the single DataFrame to a table
# single_df.write.saveAsTable("my_table")
single_df.write.csv(csv_dir, header=True)


# CELL 6 #
import pandas as pd
from pyspark.sql import SparkSession

# Create a SparkSession
spark = SparkSession.builder.appName("json combiner").config("spark.driver.memory", "128g").getOrCreate()
# spark.conf.set("spark.driver.memory", "64g")

# Define the directory containing the JSON files
json_dir = "dbfs:/Volumes/wholesale/bronze/crt_output/{str(run_date)}/3_processed/new_osp_build/"
# Define the output directory for the CSV files
csv_dir = "dbfs:/Volumes/wholesale/bronze/crt_output/{str(run_date)}/3_processed/combined_osp_csvs/"

# Define the batch size
batch_size = 1000

# Get a list of all JSON files in the directory
json_files = dbutils.fs.ls(json_dir)

# Process the JSON files in batches
for i in range(0, len(json_files), batch_size):
    batch_files = [file.path for file in json_files[i:i+batch_size]]
    batch_df = spark.read.json(batch_files)

    if "xls" in batch_df.columns:
        batch_df = batch_df.select("xls.*")
        batch_df = batch_df.selectExpr("explode(Builds) as Builds")
        batch_df = batch_df.select("Builds.*")
    else:
        # Handle the case where the "xls" column does not exist
        # For example, you can select all columns or a specific set of columns
        batch_df = batch_df.select("*")
        batch_df = batch_df.selectExpr("to_json(struct(*)) as json_data")

    batch_df.write.csv(f"{csv_dir}/batch_{i}.csv", header=True, mode="overwrite")

# Read all CSV files into a single DataFrame
csv_files = dbutils.fs.ls(csv_dir)
csv_dfs = [spark.read.csv(file.path, header=True) for file in csv_files]
single_df = csv_dfs[0]
for df in csv_dfs[1:]:
    single_df = single_df.union(df)

# Save the single DataFrame to a table
single_df.coalesce(1).write.csv(f"{csv_dir}/combined_osp_jsons_single_csv", header=True)


# CELL 7 #
from pyspark.sql import SparkSession

# Create a SparkSession
spark = SparkSession.builder.getOrCreate()

# Define the directory path
directory_path = "dbfs:/Volumes/wholesale/bronze/crt_output/{str(run_date)}/3_processed/combined_osp_csvs/combined_osp_jsons_single_csv/"

# List all files in the directory
files = dbutils.fs.ls(directory_path)

# Identify the CSV file
csv_files = [file.name for file in files if file.name.endswith(".csv")]

# Check if a CSV file exists
if csv_files:
    # Get the full path of the CSV file
    file_path = directory_path + csv_files[0]

    # Read the CSV file
    df = spark.read.csv(file_path, header=True, inferSchema=True)
    df_pandas = df.toPandas()

else:
    print("No CSV file found.")


# CELL 8 #

pd.set_option('display.max_colwidth', None)
pd.set_option('display.max_columns', None)
pd.set_option('display.width', 1000)

df_pandas.head()


# CELL 9 #

import ast

# Define the chunk size
chunksize = 10000

# Define a function to parse the JSON data
def parse_json_data(json_data):
    try:
        # Load the JSON data
        data = ast.literal_eval(json_data)

        # Get the first key (Build_ID)
        build_id = list(data.keys())[0]

        # Get the Builds data
        builds_data = data[build_id]['Builds']

        # Initialize an empty list to store the parsed data
        parsed_data = []

        # Iterate over the Builds data
        for build in builds_data:
            # Create a new dictionary to store the parsed data
            parsed_build = {
                'Build_ID': build_id,
                'Contact': build.get('Contact', None),
                'Description': build.get('Description', None),
                'Engineer': build.get('Engineer', None),
            }

            # Add the Material data
            material_keys = [key for key in build.keys() if key.startswith('Material_')]
            for key in material_keys:
                parsed_build[key] = build[key]

            # Add the Placing data
            placing_keys = [key for key in build.keys() if key.startswith('Placing_')]
            for key in placing_keys:
                parsed_build[key] = build[key]

            # Add the Splicing data
            splicing_keys = [key for key in build.keys() if key.startswith('Splicing_')]
            for key in splicing_keys:
                parsed_build[key] = build[key]

            # Add the Removal data
            removal_keys = [key for key in build.keys() if key.startswith('Removal_')]
            for key in removal_keys:
                parsed_build[key] = build[key]

            # Add the parsed build data to the list
            parsed_data.append(parsed_build)

        # Return the parsed data
        return pd.DataFrame(parsed_data)

    except Exception as e:
        # Handle any exceptions
        print(f"Error parsing JSON data: {e}")
        return None

# Apply the parse_json_data function to the 'json_data' column in chunks
parsed_data = []
for i in range(0, len(df_pandas), chunksize):
    chunk = df_pandas['json_data'].iloc[i:i+chunksize]
    parsed_chunk = chunk.apply(parse_json_data)
    parsed_data.extend([df for df in parsed_chunk if df is not None])

# Concatenate the parsed data into a single DataFrame
df_pandas = pd.concat(parsed_data, ignore_index=True)

# Print the resulting DataFrame
df_pandas.head()
